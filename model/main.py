import pandas as pd
import numpy as np

import matplotlib
matplotlib.use("TkAgg")

import matplotlib.pyplot as plt
import tensorflow as tf

# For Live graphs
import matplotlib.animation as animation
from matplotlib import style


dataframe = pd.read_csv('kddcup.10')

# Data Selection
dataframe = dataframe[0:494021]


inputX =  dataframe.loc[:,[
						  'protocol_type',
						  'service',
						  'land',
						  'count',
						  'srv_count',
						  'urgent',
						  'same_srv_rate',
						  'diff_srv_rate',
						  'srv_diff_host_rate']].as_matrix()


# Converts a given service name to a data point numerical value which is consistent throughout the model.
def convert_service_to_data_point(s_name):
	
	services = ['http', 'smtp', 'finger', 'domain_u', 'auth', 'telnet', 'ftp', 'eco_i', 'ntp_u', 'ecr_i', 'other', 'private', 'pop_3', 'ftp_data', 'rje', 'time', 'mtp', 'link', 'remote_job', 'gopher', 'ssh', 'name', 'whois', 'domain', 'login', 'imap4', 'daytime', 'ctf', 'nntp', 'shell', 'IRC', 'nnsp', 'http_443', 'exec', 'printer', 'efs', 'courier', 'uucp', 'klogin', 'kshell', 'echo', 'discard', 'systat', 'supdup', 'iso_tsap', 'hostnames', 'csnet_ns', 'pop_2', 'sunrpc', 'uucp_path', 'netbios_ns', 'netbios_ssn', 'netbios_dgm', 'sql_net', 'vmnet', 'bgp', 'Z39_50', 'ldap', 'netstat', 'urh_i', 'X11', 'urp_i', 'pm_dump', 'tftp_u', 'tim_i', 'red_i']

	if s_name in services:
		# Assumed port number for 'other'
		return services.index(s_name)
	else:
		return services.index('other')


# Convert a protocol to data point 
def convert_protocol_to_data_point(proto):
	if proto == 'tcp':
		return 6
	elif proto == 'udp':
		return 17
	elif proto == 'icmp':
		return 1


def preprocess_input(input_x):
	for x in input_x:
		#  Convert service names for each packet into data points
		x[1] = convert_service_to_data_point(x[1])
		# Convert protocol for each packet into data points
		x[0] = convert_protocol_to_data_point(x[0])
	return input_x
		

# ============================
# Data Preprocessing - inputX
# ============================
for x in inputX:
	#  Convert service names for each packet into data points
	x[1] = convert_service_to_data_point(x[1])
	# Convert protocol for each packet into data points
	x[0] = convert_protocol_to_data_point(x[0])


# ============================
# Data Preprocessing - InputY
# ============================

attacks = [
	'back',
	'buffer_overflow',
	'ftp_write',
	'guess_passwd',
	'imap',
	'ipsweep',
	'land',
	'loadmodule',
	'multihop',
	'neptune',
	'nmap',
	'normal',
	'perl',
	'phf',
	'pod',
	'portsweep',
	'rootkit',
	'satan',
	'smurf'
]
attack_ids = [0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18]
attack_dict = dict(zip(attacks, attack_ids))

dataframe['label']=dataframe['label'].map(lambda x : str(x)[:-1])
dataframe['label'] = dataframe['label'].map(attack_dict)

# Integer labels
tempInputY = dataframe['label']

# Input of labels, encoded as one hot input
inputY = []

for inY in tempInputY:
	lst = []
	for y_id in range(0, len(attacks)):
		if inY == y_id:
			lst.append(1)
		else:
			lst.append(0)
	inputY.append(lst)
# Convert one hot list to one hot array of labels
inputY = np.array(inputY)

# ======================
# Hyperparameters Setup
# ======================

parameters = {
	'learning_rate': 0.0001,
	'training_epochs': 250,
	'display_steps': 1,
	'n_features': inputX[0].size,
	'n_classes': inputY[0].size
}

# ========================
# CREATE COMPUTATION MODEL
# ========================


x = tf.placeholder(tf.float32, [None, parameters['n_features']])

# Initialize weights
W = tf.Variable(tf.zeros([parameters['n_features'], parameters['n_classes']]))

# Initialize biases
b = tf.Variable(tf.zeros([parameters['n_classes']]))

# Aply softmax activation function
y = tf.nn.softmax(tf.matmul(x, W) + b)

y_ = tf.placeholder(tf.float32, [None, parameters['n_classes']])

xpoints = []
ypoints = []

def train_and_save_model(inputX, inputY, parameters):
	# cost = tf.reduce_mean(-tf.reduce_sum(y_ * tf.log(y), reduction_indices=[1]))
	cost = tf.reduce_sum(tf.pow(y_ - y, 2)) / (2 * parameters['n_classes'])

	train_step = tf.train.AdamOptimizer(parameters['learning_rate']).minimize(cost)
	# Initiate tf saver
	saver = tf.train.Saver()
	# Model Path
	model_path = "./tmp/model.ckpt"

	# Starting Session
	sess = tf.InteractiveSession()
	tf.global_variables_initializer().run()

	for i in range(parameters['training_epochs']):
		sess.run(train_step, feed_dict={x:inputX, y_:inputY})
		cc = sess.run(cost, feed_dict={x:inputX, y_:inputY})
		if i % parameters['display_steps'] == 0:
			correct_prediction = tf.equal(tf.argmax(y,1), tf.argmax(y_,1))
			accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))
			accuracy_percentage = sess.run(accuracy, feed_dict={x: inputX, y_: inputY})
			print("Training Step: ", "%04d" % (i), 'cost=', "{:.9f}".format(cc), "Accuracy: ",accuracy_percentage )
			xpoints.append(i)
			ypoints.append(accuracy_percentage * 100)




	print("Optimization Finished!")
	print(xpoints, ypoints)
	x2points = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249]
	y2points = [73.426836729049683, 71.82164192199707, 71.596753597259521, 71.612542867660522, 71.710312366485596, 71.862733364105225, 72.065967321395874, 72.285795211791992, 72.532546520233154, 72.817349433898926, 72.596913576126099, 72.385585308074951, 72.219198942184448, 72.10766077041626, 72.066569328308105, 72.051185369491577, 72.047543525695801, 72.053617238998413, 72.082763910293579, 72.109484672546387, 72.139847278594971, 72.167176008224487, 72.191464900970459, 72.209280729293823, 72.219198942184448, 72.227495908737183, 72.230130434036255, 72.229117155075073, 72.227495908737183, 72.224056720733643, 72.213733196258545, 72.204017639160156, 72.193694114685059, 72.180533409118652, 72.166770696640015, 72.149562835693359, 72.129321098327637, 72.105437517166138, 72.085195779800415, 72.053819894790649, 72.028112411499023, 71.995115280151367, 71.962529420852661, 71.931356191635132, 71.896135807037354, 71.861118078231812, 71.835207939147949, 71.807068586349487, 71.780955791473389, 71.756464242935181, 71.731770038604736, 71.705454587936401, 71.685212850570679, 71.664363145828247, 71.640276908874512, 71.623879671096802, 71.614569425582886, 71.5923011302948, 71.575701236724854, 71.565580368041992, 71.557283401489258, 71.539264917373657, 71.526515483856201, 71.511536836624146, 71.49655818939209, 71.473073959350586, 71.459108591079712, 71.443116664886475, 71.429556608200073, 71.40546441078186, 71.368223428726196, 71.339678764343262, 71.31195068359375, 71.288871765136719, 71.272677183151245, 71.258711814880371, 71.248996257781982, 71.242314577102661, 71.237051486968994, 71.229970455169678, 71.228146553039551, 71.228349208831787, 71.22875452041626, 71.230572462081909, 71.236848831176758, 71.242111921310425, 71.248793601989746, 71.260738372802734, 71.274501085281372, 71.287047863006592, 71.30243182182312, 71.320044994354248, 71.338868141174316, 71.361744403839111, 71.386843919754028, 71.403849124908447, 71.41498327255249, 71.423280239105225, 71.434009075164795, 71.442711353302002, 71.451616287231445, 71.45809531211853, 71.465587615966797, 71.475505828857422, 71.486836671829224, 71.497970819473267, 71.508091688156128, 71.515989303588867, 71.522063016891479, 71.529346704483032, 71.534204483032227, 71.538251638412476, 71.547162532806396, 71.55383825302124, 71.557283401489258, 71.563959121704102, 71.570640802383423, 71.578943729400635, 71.590679883956909, 71.598982810974121, 71.603232622146606, 71.608293056488037, 71.614772081375122, 71.624284982681274, 71.637237071990967, 71.646547317504883, 71.654850244522095, 71.664363145828247, 71.677112579345703, 71.69533371925354, 71.706873178482056, 71.719014644622803, 71.7315673828125, 71.748971939086914, 71.773874759674072, 71.790468692779541, 71.803426742553711, 71.820425987243652, 71.839660406112671, 71.857267618179321, 71.874678134918213, 71.897143125534058, 71.922850608825684, 71.948361396789551, 71.974873542785645, 71.997547149658203, 72.024065256118774, 72.057259082794189, 72.086411714553833, 72.112524509429932, 72.142887115478516, 72.1732497215271, 72.198957204818726, 72.239440679550171, 72.271424531936646, 72.30522632598877, 72.339838743209839, 72.374856472015381, 72.416150569915771, 72.454410791397095, 72.492259740829468, 72.530114650726318, 72.571206092834473, 72.614729404449463, 72.649341821670532, 72.686386108398438, 72.726058959960938, 72.767961025238037, 72.820585966110229, 72.865122556686401, 72.913700342178345, 72.959041595458984, 73.207211494445801, 73.256599903106689, 73.302149772644043, 73.357206583023071, 73.408013582229614, 73.458820581436157, 73.519545793533325, 73.582702875137329, 73.66144061088562, 73.788970708847046, 74.137133359909058, 74.485296010971069, 74.842971563339233, 75.592738389968872, 76.486831903457642, 77.466946840286255, 78.113275766372681, 79.159188270568848, 80.016231536865234, 81.032383441925049, 81.939429044723511, 82.870364189147949, 83.827000856399536, 84.807932376861572, 85.813355445861816, 86.708259582519531, 87.831288576126099, 88.803106546401978, 89.781808853149414, 90.540480613708496, 91.199767589569092, 91.794884204864502, 92.221587896347046, 92.645251750946045, 93.048268556594849, 93.3624267578125, 93.662619590759277, 93.967866897583008, 94.283443689346313, 94.529992341995239, 94.710749387741089, 94.864386320114136, 94.964182376861572, 95.062965154647827, 95.132189989089966, 95.201420783996582, 95.257288217544556, 95.302629470825195, 95.333397388458252, 95.358496904373169, 95.376914739608765, 95.387238264083862, 95.402622222900391, 95.417201519012451, 95.426511764526367, 95.434200763702393, 95.439869165420532, 95.445740222930908, 95.450598001480103, 95.456063747406006, 95.462137460708618, 95.467400550842285, 95.473271608352661, 95.47913670539856, 95.486831665039062, 95.491689443588257, 95.507478713989258, 95.501202344894409, 95.507681369781494, 95.511525869369507, 95.511728525161743, 95.511728525161743, 95.512336492538452, 95.511525869369507, 95.512741804122925, 95.515167713165283, 95.517599582672119, 95.520633459091187] 

	xmlp = [1,2,3,4,5,6,7,8,9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404, 405, 406, 407, 408, 409, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419, 420, 421, 422, 423, 424, 425, 426, 427, 428, 429, 430, 431, 432, 433, 434, 435, 436, 437, 438, 439, 440, 441, 442, 443, 444, 445, 446, 447, 448, 449, 450, 451, 452, 453, 454, 455, 456, 457, 458, 459, 460, 461, 462, 463, 464, 465, 466, 467, 468, 469, 470, 471, 472, 473, 474, 475, 476, 477, 478, 479, 480, 481, 482, 483, 484, 485, 486, 487, 488, 489, 490, 491, 492, 493, 494, 495, 496, 497, 498, 499, 500, 501, 502, 503, 504, 505, 506, 507, 508, 509, 510, 511, 512, 513, 514, 515, 516, 517, 518, 519, 520, 521, 522, 523, 524, 525, 526, 527, 528, 529, 530, 531, 532, 533, 534, 535, 536, 537, 538, 539, 540, 541, 542, 543, 544, 545, 546, 547, 548, 549, 550, 551, 552, 553, 554, 555, 556, 557, 558, 559, 560, 561, 562, 563, 564, 565, 566, 567, 568, 569, 570, 571, 572, 573, 574, 575, 576, 577, 578, 579, 580, 581, 582, 583, 584, 585, 586, 587, 588, 589, 590, 591, 592, 593, 594, 595, 596, 597, 598, 599, 600, 601, 602, 603, 604, 605, 606, 607, 608, 609, 610, 611, 612, 613, 614, 615, 616, 617, 618, 619, 620, 621, 622, 623, 624, 625, 626, 627, 628, 629, 630, 631, 632, 633, 634, 635, 636, 637, 638, 639, 640, 641, 642, 643, 644, 645, 646, 647, 648, 649, 650, 651, 652, 653, 654, 655, 656, 657, 658, 659, 660, 661, 662, 663, 664, 665, 666, 667, 668, 669, 670, 671, 672, 673, 674, 675, 676, 677, 678, 679, 680, 681, 682, 683, 684, 685, 686, 687, 688, 689, 690, 691, 692, 693, 694, 695, 696, 697, 698, 699, 700, 701, 702, 703, 704, 705, 706, 707, 708, 709, 710, 711, 712, 713, 714, 715, 716, 717, 718, 719, 720, 721, 722, 723, 724, 725, 726, 727, 728, 729, 730, 731, 732, 733, 734, 735, 736, 737, 738, 739, 740, 741, 742, 743, 744, 745, 746, 747, 748, 749, 750]
	ymlp = [0.0392338, 0.0392338, 0.0392545, 0.0392545, 0.0392545, 0.0392958, 0.0392958, 0.0392752, 0.0392752, 0.0392752, 0.0392752,0.0392958,0.0392752,0.0392752,0.0392752,0.0392752,0.0393165,0.0393165,0.0393165,0.0393165,0.0393165,0.0393165,0.0393371,0.0393371,0.0393371,0.0393371,0.0393578,0.0393371,0.0393578,0.0393578,0.0393785,0.0393785,0.0393785,0.0393991,0.0393991,0.0393991,0.0393991,0.0393991,0.0393991,0.0393991,0.0393991,0.0394198,0.0394198,0.0394198,0.0394198,0.0394404,0.0394404,0.0394404,0.0394404,0.0394404,0.0394404,0.0394404,0.0394611,0.0395024,0.0395024,0.0395024,0.0395024,0.0395024,0.0395024,0.0395024,0.0395024,0.0395024,0.0395024,0.0395231,0.0395231,0.0395231,0.0395437,0.0395851,0.0395851,0.0396057,0.0396264,0.039647,0.039709,0.039709,0.039709,0.039709,0.039709,0.0396884,0.0396884,0.0396884,0.039709,0.0397297,0.0397503,0.0397503,0.0397503,0.0397503,0.0397503,0.039771,0.0397917,0.0398123,0.0398123,0.039833,0.039833,0.039833,0.039833,0.0398743,0.0398743,0.0399156,0.0399156,0.0399156,0.0399156,0.0399569,0.0399983,0.0399983,0.0400189,0.0400189,0.0400396,0.0400396,0.0400396,0.0400809,0.0401016,0.0401222,0.0401222,0.0401429,0.0402049,0.0402255,0.0402255,0.0402462,0.0402668,0.0403082,0.0403702,0.0404321,0.0404321,0.0404528,0.0404734,0.0404941,0.0405561,0.0406181,0.0406387,0.0407007,0.040742,0.0407627,0.040804,0.040928,0.0409693,0.04099,0.0410519,0.0411139,0.0411139,0.0412379,0.0413205,0.0413618,0.0414238,0.0414238,0.0414651,0.0416304,0.0418164,0.0418783,0.0419197,0.042023,0.0420436,0.0420436,0.0421469,0.0423535,0.0423742,0.0423742,0.0423742,0.0425808,0.0425808,0.0426015,0.0426221,0.0429527,0.0430973,0.043118,0.0431386,0.0431386,0.0433452,0.0433865,0.0434485,0.0435312,0.0436138,0.0437171,0.0437998,0.0439237,0.043965,0.044089,0.0441716,0.0443163,0.0444402,0.0444402,0.0445229,0.0445848,0.0446881,0.0448121,0.0449567,0.044998,0.0450807,0.0451013,0.045246,0.0453699,0.0453699,0.0455146,0.0455972,0.0456385,0.0458451,0.0459484,0.0460724,0.0460724,0.0461344,0.0461344,0.046155,0.0462377,0.0464029,0.0464029,0.0465062,0.0465682,0.0466095,0.0466715,0.0466715,0.0467128,0.0467955,0.0468368,0.0469401,0.0470228,0.0470021,0.0470021,0.0470641,0.0470847,0.0471467,0.0472294,0.047312,0.0474153,0.0474773,0.0474773,0.0475186,0.0476012,0.0476839,0.0477665,0.0477665,0.0477872,0.0479111,0.0479111,0.0480558,0.0482004,0.0482417,0.048283,0.048345,0.048407,0.048407,0.0484277,0.0484277,0.0486549,0.0486962,0.0487169,0.0487789,0.0488822,0.0488822,0.0489028,0.0489442,0.0489442,0.0490268,0.0490475,0.0490888,0.0490888,0.0491301,0.0491714,0.0491921,0.0492127,0.0495846,0.0496673,0.0496879,0.0497706,0.0497912,0.0498532,0.0498945,0.0499152,0.0500598,0.0500598,0.0500805,0.0502044,0.0503697,0.0504317,0.0504317,0.0505143,0.0505143,0.050597,0.0507003,0.0507209,0.0507209,0.0507829,0.0507829,0.0507829,0.0507829,0.0507829,0.0508656,0.0510102,0.0510722,0.0511135,0.0511548,0.0511755,0.0511755,0.0511755,0.0513821,0.0513821,0.0513821,0.0514234,0.051444,0.0514647,0.0514647,0.0514647,0.0516093,0.0517746,0.0518366,0.0519192,0.0519192,0.0519812,0.0519812,0.0519812,0.0520432,0.0522291,0.0522291,0.0522498,0.0522705,0.0523118,0.0523118,0.0523118,0.0523324,0.0524771,0.0524771,0.0526837,0.0527456,0.052787,0.052787,0.0528076,0.0529522,0.0529522,0.0530555,0.0531175,0.0531588,0.0534274,0.0534274,0.0534481,0.0535514,0.053634,0.0536547,0.0536547,0.0539026,0.0539026,0.0541092,0.0543158,0.0543571,0.0543985,0.0545018,0.0545431,0.0545637,0.0546051,0.0546877,0.0548117,0.0548323,0.0548323,0.054853,0.0548737,0.0548943,0.0549563,0.054977,0.0551836,0.0554108,0.0554728,0.0555348,0.0555554,0.0555968,0.0557414,0.0559067,0.0559067,0.0559893,0.0561133,0.0561752,0.0562786,0.0563818,0.0564025,0.0564232,0.0564232,0.0566091,0.0567124,0.0567124,0.0567537,0.0568157,0.0568157,0.0568157,0.0568777,0.056919,0.0569603,0.056981,0.0570017,0.0571256,0.0571256,0.0571876,0.0572496,0.0572909,0.0573529,0.0574149,0.0574562,0.0574768,0.0575182,0.0575182,0.0575388,0.0575801,0.0576008,0.0577041,0.0577454,0.0577454,0.0578487,0.0578694,0.058014,0.0580967,0.0582826,0.0583033,0.0583446,0.0585099,0.0586338,0.0588404,0.0588817,0.0589437,0.059171,0.059233,0.0592536,0.0592949,0.0593569,0.0593982,0.0594602,0.0601214,0.0602453,0.060266,0.0603899,0.0604313,0.0604519,0.0604932,0.0605552,0.0605965,0.0606379,0.0606792,0.0606998,0.0607618,0.0608031,0.0608651,0.0609064,0.0609478,0.0609684,0.0609684,0.0611337,0.0611337,0.0612164,0.061237,0.0612783,0.0613816,0.0614849,0.0615056,0.0615056,0.0615882,0.0616709,0.0617329,0.0618155,0.0618362,0.0618362,0.0619808,0.0619808,0.0620014,0.0621047,0.0621461,0.0621461,0.0622494,0.0623113,0.0623113,0.0625386,0.0625593,0.0626832,0.0628692,0.0629725,0.0629931,0.0630758,0.0630964,0.0631378,0.0631584,0.0632411,0.063303,0.063551,0.0640881,0.0641914,0.0642327,0.0642534,0.0644187,0.064522,0.0645427,0.0647079,0.0648112,0.0648939,0.0650798,0.0651211,0.0651211,0.0651211,0.0651211,0.0651211,0.0652038,0.0653484,0.065431,0.065555,0.0656376,0.0657616,0.0658236,0.0658442,0.0660095,0.0660509,0.0661955,0.0662575,0.0662781,0.0664227,0.0665054,0.06665,0.0667533,0.0668153,0.0668773,0.0670632,0.0671458,0.0674144,0.0674557,0.0676004,0.067745,0.0678896,0.0680136,0.0680962,0.0682202,0.0683235,0.0683441,0.0685094,0.0685921,0.068654,0.0686954,0.0688193,0.0688607,0.0689846,0.0690673,0.0691912,0.0693565,0.0693772,0.0695011,0.0695631,0.0696251,0.0699143,0.0700176,0.0701209,0.0701622,0.0703069,0.0705135,0.0707407,0.0708234,0.0712986,0.0713812,0.0715671,0.0717531,0.0718151,0.072063,0.0722283,0.0723316,0.0724969,0.0726621,0.0727448,0.0729927,0.073096,0.073158,0.0732613,0.0734059,0.0736745,0.0736952,0.0737985,0.0739018,0.0740464,0.0742736,0.074315,0.0745629,0.0746662,0.0747488,0.0750174,0.0751,0.0751827,0.0752033,0.0753686,0.0755752,0.0758645,0.0759678,0.0760504,0.0763603,0.0764636,0.0765463,0.0768148,0.0769182,0.0769801,0.0772074,0.0772487,0.077352,0.0776206,0.0777446,0.0779098,0.0779718,0.0780545,0.0781371,0.078509,0.0785297,0.0786949,0.0786949,0.0788809,0.0793974,0.079666,0.0798106,0.0799139,0.0800998,0.0801412,0.0806577,0.0808436,0.0808436,0.0811948,0.0813601,0.0815047,0.0819593,0.0821865,0.0822485,0.0824551,0.0827443,0.0830542,0.0835501,0.0838187,0.0840459,0.0843352,0.0845005,0.084893,0.0855541,0.0856574,0.0858847,0.0860087,0.0863392,0.0864632,0.0867731,0.087145,0.0877028,0.0878474,0.088116,0.0883639,0.0887978,0.0890457,0.0895209,0.0896242,0.0898721,0.0900581,0.0904713,0.0907812,0.0912357,0.0916076,0.0921034,0.0925373,0.0928266,0.0931778,0.0936323,0.0940248,0.0943761,0.0945413,0.0948099,0.0951198,0.0954091,0.0960909,0.0963595,0.0969793,0.0973718,0.0980536,0.0985701,0.0993552,0.100285,0.101049,0.102268,0.103487,0.104479,0.105615,0.107371,0.109458,0.110429,0.113012,0.116276,0.118941,0.121131,0.123362,0.125945,0.128465,0.131255,0.135077,0.137845,0.141048,0.144271,0.148713,0.150923,0.155179,0.158877,0.162059,0.165943,0.170633,0.175426,0.180447,0.184413,0.188422,0.193979,0.20088,0.204247,0.208937,0.215301,0.221561,0.227924,0.233523,0.238213,0.24406,0.251518,0.259204,0.264575,0.270443,0.277529,0.285855,0.293004,0.299285,0.305854,0.316577,0.336928,0.349882,0.364034,0.386016,0.40335,0.423597,0.470248,0.516858,0.554625,0.609292,0.647369,0.701622,0.753232,0.787652,0.85019,0.909279,0.928285,0.929385,0.938399,0.939285,0.948285,0.958285,0.958285,0.958285,0.958285,0.958285,0.958285,0.958285,0.958285,0.958285,0.958285,0.958285,0.958285,0.958285]
	
	plt.plot(xpoints, ypoints, label='SF 1')
	plt.plot(x2points, y2points,  label='SF 2')
	plt.plot(xmlp, ymlp, label= 'MLP')

	plt.xlabel('Epochs')
	plt.ylabel('Accuracy Percentage')
	plt.title("Model Accuracy vs No of Epochs")
	plt.legend()
	plt.show()
	# training_cost = sess.run(cost, feed_dict={x:inputX, y_:inputY})
	# print("Training cost= ", training_cost, " W=", sess.run(W), " b=", sess.run(b))

	save_path = saver.save(sess, model_path)
	print("Model saved in file: %s" % save_path)
	return save_path


def predict_class(input_x, save_path):
	sess = tf.InteractiveSession()
	saver = tf.train.Saver()

	saver.restore(sess, save_path)
	print("Model restored from file: %s" % save_path)
	
	feed_dict = {x: input_x}
	classification = list(sess.run(y, feed_dict))
	# print(classification)
	for c in classification:
		c = list(c)
		print(attacks[c.index(max(c))])



def main():
	save_path = train_and_save_model(inputX, inputY, parameters)
	# print(predict_x)

	# predict_x = dataframe.loc[397000:397100,[
	# 					  'protocol_type',
	# 					  'service',
	# 					  'land',
	# 					  'count',
	# 					  'srv_count',
	# 					  'urgent',
	# 					  'same_srv_rate',
	# 					  'diff_srv_rate',
	# 					  'srv_diff_host_rate']].as_matrix()
	# predict_x = preprocess_input(predict_x)
	# predict_class(predict_x, './tmp/model.ckpt')

if __name__ == '__main__':
	main()