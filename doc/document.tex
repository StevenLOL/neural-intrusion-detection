% !TeX spellcheck = en_GB
\documentclass[12pt]{article}

\usepackage{graphicx}
\usepackage{fancyhdr}
\usepackage{lipsum}
% For theorems
\usepackage{amsthm}

% For Code
\usepackage{listings} 

% For Acronyms
\usepackage{acronym}

% Bibliography preamble
\usepackage[numbers,sort&compress]{natbib}

% Project Title
\newcommand{\projectTitle}{Host Based Intrusion Detection and Classification using Neural Networks}

% Macro for inserting quotes
\newcommand{\quotes}[1]{``#1''}

% Code Settings
\lstset{
	basicstyle=\ttfamily\small,
	basewidth=0.55em,
	showstringspaces=false,
	numbers=left,
	numberstyle=\tiny,
	numbersep=2.5pt,
	keywordstyle=\bfseries\ttfamily,
	breaklines=true
}
\lstnewenvironment{logs}{\lstset{frame=lines,basicstyle=\footnotesize\ttfamily,numbers=none}}{}

%Header / Footer

\pagestyle{fancy}
\fancyhead{} % Clear the header
\fancyfoot{} % Clear the footer

\fancyfoot[R]{ \thepage\ }
\renewcommand{\headrulewidth}{0pt}
\renewcommand{\footrulewidth}{0pt}

\usepackage[margin=1in,left=1.5in,includefoot]{geometry}
\theoremstyle{definition}
\newtheorem{example}{Example}[section]

\begin{document}

	% TITLE PAGE
	\begin{titlepage}
		\vspace*{\fill}
		\begin{center}
			\textsc{\Large \projectTitle}\\
			[0.5in]
			B.Tech Major Project Report\\
			[2in]
			By\\
			[.5in]
			\textsc{Anand Amrit Raj}\\
			Supervised By : Ms. Veena Anand\\
			[.5in]
			\begin{figure}[!h]
				\centering
				\includegraphics[width=100pt]{pictures/nitrr-logo.jpg}
			\end{figure}
			\vspace{.25in}
			\textsc{Department of Computer Sciene and Enginnering}\\
			\textsc{National Institue of technology}\\
			\textsc{Raipur, CG (India)}\\
			MAY, 2017
		\end{center}
	 	\vspace*{\fill}
	\end{titlepage}
	% TITLE PAGE ENDS
	
	% SECOND PAGE
	\begin{titlepage}
		\vspace*{\fill}
		\begin{center}
			\textsc{\Large \projectTitle}\\
			[0.5in]
			\textbf{A Major Project Report}\\
			[.25in]
			\textit{submitted in partial fulfilment of the} \\
			\textit{requirements for the award of the degree}\\
			[.25in]
			of\\
			[.25in]
			Bachelor of Technology \\
			[.25in]
			in\\
			[.25in]
			Computer Science and Enginneering\\
			By\\
			[.5in]
			\textsc{Anand Amrit Raj}\\
			Supervised By : Ms. Veena Anand\\
			[.5in]
			\begin{figure}[!h]
				\centering
				\includegraphics[width=100pt]{pictures/nitrr-logo.jpg}
			\end{figure}
			\vspace{.25in}
			\textsc{Department of Computer Sciene and Enginnering}\\
			\textsc{National Institue of technology}\\
			\textsc{Raipur, CG (India)}\\
			MAY, 2017
		\end{center}
		\vspace*{\fill}
	\end{titlepage}
	
	% SECOND PAGE	
	
	
	%CERTIFICATE
	\begin{titlepage}
		
		\textbf{%\hspace*{-1pt} 
			%\includegraphics[width=0.15\textwidth]{./img/logo.png}\hfill 
			\makebox[0pt][l]{\includegraphics[width=0.20\textwidth]{pictures/nitrr-logo.jpg}}
			\hfill
			\Large Certificate \hfill\hfill\\
			%\hspace*{-100pt} This Text}}
		}
		\vspace{15pt}
		
		I hereby certify that the work which is being presented in the B.Tech. Major Project Report entitled \quotes{\textbf{\projectTitle}}, in partial fulfillment of the requirements for the award of the Bachelor  of Technology in Computer Science \& Engineering and submitted to the Department of Computer Science \& Engineering of National Institute of Technology Raipur  is an authentic record of my own work carried out during a period from July 2016 to December 2016 under the supervision of  Ms. Veena Anand, Assistant Professor, CSE Department. 

		The matter presented in this thesis has not been submitted by me for the award of any other degree elsewhere.\\
		[.25in]
		\begin{flushright}
			\textit{Signature of Candidate}\\
				\textbf{Anand Amrit Raj \\
				Roll Number 13115006\\}
		\end{flushright}
	\vspace{10pt}
	This is to certify that the above statement made by the candidate is correct to the best of my knowledge.

		\begin{flushleft}
			\textbf{Date:}
		\end{flushleft}
		\begin{flushright}
			\textit{Signature of Supervisor}\\
				\textbf{Ms. Veena Anand, Asst. Professor\\
				Project Supervisor\\}
		\end{flushright}
		\vspace{30pt}
		\begin{flushleft}
			\textbf{\large Head}\\
			\textbf{Computer Science and Engineering}\\
			National Institute of Technology, Raipur, CG
		\end{flushleft}
	\end{titlepage}
	%CERTIFICATE
	
	%ABSTRACT
	\pagenumbering{roman}
	\section*{\centering Abstract}
	\addcontentsline{toc}{section}{\numberline{}Abstract}
	
	\lipsum[1-4]
	\cleardoublepage
	%ABSTRACT ENDS
	
	
	%ACKNOWLEDGEMENT
	\section*{\centering Acknowledgement}
	\addcontentsline{toc}{section}{\numberline{}Acknowledgement}
	
	I express my sincere thanks and gratitude to my guide Ms. Veena Anand, Assistant Professor, Department of Computer Science \& Engineering, NIT Raipur for her guidance and supervision throughout the project. My sincere regards to Dr. Dilip Singh Sisodia Sir, Head of Department, Computer Science \& Engineering, NIT Raipur.\\
	
	I express my deep sense of gratitude to Director, NIT Raipur.  I am also thankful to all the other faculties of CSE Department and my colleagues at college who helped me to complete my project. I thank all my friends for their consistent support throughout the project term.
	
	\vspace{10pt}
	
	\begin{flushleft}
	Anand Amrit Raj\\
	B. Tech Computer Science and Engineering
	\end{flushleft}
	\cleardoublepage
	%ACKNOWLEDGEMENT ENDS
	
	
	% TABLE OF CONTENTS
	\tableofcontents{}
	\thispagestyle{empty}
	\cleardoublepage
	% TABLE OF CONTENTS ENDS
		
	%LIST OF FIGURES
	\listoffigures
	\addcontentsline{toc}{section}{\numberline{}List of Figures}
	\cleardoublepage
	%LIST OF FIGURES ENDS
	
	%LIST OF TABLES
	\listoftables
	\addcontentsline{toc}{section}{\numberline{}List of Tables}
	\cleardoublepage
	%LIST OF TABLES ENDS
	
	\pagenumbering{arabic}
	\setcounter{page}{1}
	
	% Introduction
	\cleardoublepage
	\section{Introduction}\label{sec:intro}
		
		Network connected devices such as personal computers, smart phones, or gaming consoles are nowadays enjoying immense popularity. In parallel, the Web and the humongous amount of services it offers have certainly became the most ubiquitous tools of all the times. Facebook counts more than 1.28 billion daily active users as of 31st March 2017. Approximately 70\% of the traffic generated on Facebook is through mobile devices. Every second 44,000 GBs worth of data is flowing through the internet. Google has estimated that the number of \textit{Unique Uniform Resource Locators}(URLs) is 50 trillion. Not only the Web 2.0 has became predominant; in fact, thinking that on December 1990 the Internet was made of \textit{one} site and today it counts more than 100 billion websites is just astonishing.
		
		The Internet and the Web are huge~\cite{torpig}. The relevant fact, however, is that they both became the most advanced workplace. Almost every industry connected its own network to the Internet and relies on these infrastructures for a vast majority of transactions; most of the time monetary transactions. As an example,every year \textsf{Google} looses approximately 110 millions of USDollars in ignored ads because of the \emph{``I'm feeling lucky''} button. The scary part is that, during their daily work activities, people typically pay poor or no attention at all to the risks that derive from exchanging any kind of information over such a complex, interconnected infrastructure. This is demonstrated by the effectiveness of social engineering~\cite{deception} scams carried over the Internet or the phone~\cite{social-engineering-fundamentals}. Recall that 76\% of the phishing is related to finance. Now, compare this landscape to what the most famous security quote states.
		 
		 \begin{quotation}
		 	``The only truly secure computer is one buried in concrete, with the power turned off and the network cable cut''.
		 	---\emph{Anonymous}
		 \end{quotation}
		
		In fact, the Internet is all but a safe place~\cite{whid}, with more than 1,250 \emph{known} data breaches between 2005 and 2009 \cite{breach-data} and an estimate of 7,094,922,061 records stolen by intruders since 2013. Only 4\% of these breaches where secure breaches where encryption was used and stolen data was rendered useless \cite{breach-data}. One may wonder why the advance of research in computer security and the increased awareness of governments and public institutions are still not capable of avoiding such incidents. Besides the fact that the aforementioned numbers would be order of magnitude higher in absence of countermeasures, todays' security issues are, basically, caused by the combination of two phenomena: the high amount of software vulnerabilities and the effectiveness of todays' exploitation strategy.
		
		
		\begin{description}
			\item[software flaws] --- (un)surprisingly, software is affected byvulnerabilities. Incidentally, tools that have to do with the Web,
			namely, browsers and 3\textsuperscript{rd}-party extensions, and web
			applications, are the most vulnerable ones. Everyday browsers and websites are put through extreme testing to make sure the end user is secure. Testing becomes even more difficult due to presence of various Operating Systems (OS) and a plethora of web-browsers.
			
			\item[massification of attacks] --- in parallel to the explosion of the Web 2.0, attackers and the underground economy have quickly learned that a sweep of exploits run against \emph{every} reachable host have more chances to find a vulnerable target and, thus, is much more profitable compared to a single effort to break into a high-value, well-protected machine.
		\end{description}
		
		These circumstances have initiated a vicious circle that provides the attackers with a very large pool of vulnerable targets. Vulnerable client hosts are compromised to ensure virtually unlimited bandwidth and computational resources to attackers, while server side applications are violated to host malicious code used to infect client visitors. And so forth. An old fashioned attacker would have violated a single site using all the resources available, stolen data and sold it to the underground market. Instead, a modern attacker adopts a vampire'' approach and exploit client-side software vulnerabilities to take (remote) control of million hosts. In the past the diffusion of malicious code such as viruses was sustained by sharing of infected, cracked software through floppy or compact disks; nowadays, the Web offers unlimited, public storage to attackers that deploy their exploit on compromised websites.
 		 Thus, not only the type of vulnerabilities has changed, posing virtually every interconnected device at risk. The exploitation strategy created new types of threats that take advantage of classic malicious code patterns but in a new, extensive, and tremendously effective way.
		
		
		
		\subsection{Security Threats}\label{intro:threats}
		Every year, new threats are discovered and attacker take advantage of them until effective countermeasures are found. Then, new threats are discovered, and so forth. \textsf{Symantec} quantifies the amount of new malicious code threats to be 357M as of 2017 \cite{symantec_threat_report_2017}. Thus, countermeasures must advance at least with the same grow rate.\\
		
		Todays' underground economy run a very proficient market: everyone can buy credit card information for as low as \$0.06--\$30, full identities for just \$0.70--\$60 or rent a scam hosting solution for \$3--\$40 per week plus \$2-\$20 for the design~\cite{symantec_threat_report_2017}.\\ 	 

		The main underlying technology actually employs a classic type of software called \emph{bot} (jargon for \emph{robot}), which is not malicious \emph{per s\'e}, but is used to remotely control a network of compromised hosts, called \emph{botnet}~\cite{holz}. Remote commands can be of any type and typically include launching an attack, starting a phishing or spam campaign, or even updating to the latest version of the bot software by downloading the binary code from a host controlled by the attackers (usually called \emph{bot master})~\cite{torpig}. The exchange good has now become the botnet infrastructure itself rather than the data that can be stolen or the spam that can be sent. These are mere outputs of todays' most popular service offered for rent by the underground economy.
		
		\subsubsection{Role of Intrusion Detection}
		
		The aforementioned, dramatic big picture may lead to think that the malicious software will eventually proliferate at every host of the Internet and no effective re-mediation exists. However, a more careful analysis reveals that, despite the complexity of this scenario, the problems that must be solved by a security infrastructure can be decomposed into relatively simple tasks that, surprisingly, may already have a solution. Let us look at an example.
			
		\begin{example}
				This is how a sample exploitation can be structured
				\begin{description}
					\item [injection] --- a malicious request is sent to the vulnerable
					web application with the goal of corrupting all the responses sent
					to legitimate clients from that moment on. For instance, more than
					one releases of the popular \textsf{WordPress} blog application are
					vulnerable to injection
					attacks\footnote{http://secunia.com/advisories/23595} that allow an
					attacker to permanently include arbitrary content to the
					pages. Typically, such an arbitrary content is malicious code (e.g.,
					JavaScript, VBSCrip, ActionScript, ActiveX) that, every time a
					legitimate user requests the infected page, executes on the client
					host.
					\item [infection] --- Assuming that the compromised site is
					frequently accessed --- this might be the realistic case of the
					\textsf{WordPress}-powered \textsf{ZDNet} news
					blog\footnote{http://wordpress.org/showcase/zdnet/} --- a
					significant amount of clients visit it. Due to the high popularity
					of vulnerable browsers and plug-ins, the client may run
					\textsf{Internet Explorer} --- that is the most popular --- or an
					outdated release of \textsf{Firefox} on \textsf{Windows}. This
					create the perfect circumstances for the malicious page to
					successfully execute. In the best case, it may download a virus or a
					generic malware from a website under control of the attacker, so
					infecting the machine. In the worst case, this code may also exploit
					specific browser vulnerabilities and execute in privileged mode.
					\item [control \& use] --- The malicious code just download installs
					and hides itself onto the victim's computer, which has just joined a
					botnet. As part of it, the client host can be remotely controlled by
					the attackers who can, for instance, rent it, use its bandwidth and
					computational power along with other computers to run a distributed
					(DoS) attack. Also, the host can be used to automatically perform
					the same attacks described above against other vulnerable web
					applications. And so forth.
				\end{description}
			\end{example}
		
		This simple yet quite realistic example shows the various kinds of
		malicious activity that are generated during a typical drive-by
		exploitation. It also shows its requirements and assumptions that must
		hold to guarantee success. More precisely, we can recognize:
		
		\begin{description}
			\item[network activity] --- clearly, the whole interaction relies on a
			network connection over the Internet: the \ac{HTTP} connections
			used, for instance, to download the malicious code as well as to
			launch the injection attack used to compromise the web server.
			\item[host activity] --- similarly to every other type of attack
			against an application, when the client-side code executes, the
			browser (or one of its extension plug-ins) is forced to behave
			improperly. If the malicious code executes till completion the
			attack succeeds and the host is infected. This happens only if the
			platform, operating system, and browser all match the requirements
			assumed by the exploit designer. For instance, the attack may
			succeed on \textsf{Windows} and not on \textsf{Mac OS X}, although
			the vulnerable version of, say, \textsf{Firefox} is the same on both
			the hosts.
			\item[HTTP traffic] --- in order to exploit the vulnerability of the
			web application, the attacking client must generate malicious
			HTTP requests. For instance, in the case of an SQL
			injection --- that is the second most common vulnerability in a web
			application --- instead of a regular
			
			\begin{logs}
GET /index.php?username=myuser 
			\end{logs}
			
			\noindent the web server might be forced to process a
			
			\begin{logs}
GET /index.php?username=' OR 'x'='x'--\&content=<script
src="evil.com/code.js">
			\end{logs}
			
			\noindent that causes the \texttt{index.php} page to behave
			improperly.
		\end{description}
		
		It is now clear that protection mechanisms that analyse the network
		traffic, the activity of the client's operating system, the web
		server's HTTP logs, or any combination of the three, have chances
		of recognizing that something malicious is happening in the
		network. For instance, if the ISP network adopt \textsf{Snort}, a
		lightweight IDS that analyses the network traffic for known
		attack patterns, could block all the packets marked as
		suspicious. This would prevent, for instance, the SQL injection
		to reach the web application. A similar protection level can be
		achieved by using other tools such as \textsf{ModSecurity}
		. One of the problems that may arise with
		these classic, widely adopted solutions is if a zero day\index{0-day}
		attack is used. A zero day attack or threat exploits a vulnerability
		that is unknown to the public, undisclosed to the software vendor, or
		a fix is not available; thus, protection mechanisms that merely
		blacklist known malicious activity immediately become ineffective. In
		a similar vein, if the client is protected by an anti-virus, the
		infection phase can be blocked. However, this countermeasure is once
		again successful only if the anti-virus is capable of recognizing the
		malicious code, which assumes that the code is known to be malicious.
		
		Ideally, an effective and comprehensive countermeasure can be achieved
		if all the protection tools involved (e.g., client-side,
		server-side, network-side) can collaborate together. For
		instance, if a website is publicly reported to be malicious, a
		client-side protection tool should block all the content
		downloaded from that particular website. This is only a simple
		example.
		
		Thus, countermeasures against todays' threats already exist but are
		subject to at least two drawbacks:
		
		\begin{itemize}
			\item they offer protection only against known threats. To be
			effective we must assume that all the hostile traffic can be
			enumerated, which is clearly an impossible task.
			
			\begin{quotation}
				Why is ``Enumerating Badness'' a dumb idea? It's a dumb idea
				because sometime around 1992 the amount of Badness in the Internet
				began to vastly outweigh the amount of Goodness. For every
				harmless, legitimate, application, there are dozens or hundreds of
				pieces of malware, worm tests, exploits, or viral code. Examine a
				typical antivirus package and you'll see it knows about 75,000+
				viruses that might infect your machine. Compare that to the
				legitimate 30 or so apps that I've installed on my machine, and
				you can see it's rather dumb to try to track 75,000 pieces of
				Badness when even a simpleton could track 30 pieces of
				Goodness~\citep{ranum-myths}.
			\end{quotation}
			
			\item they lack of cooperation, which is crucial to detect global and
			slow attacks.
		\end{itemize}
		
		This said, we conclude that classic approaches such as dynamic and
		static code analysis and IDS already offer good protection but
		industry and research should move toward methods that require little
		or no knowledge. In this work, we indeed focus on the so called
		anomaly-based approaches, i.e., those that attempt to recognize the
		threats by detecting any variation from a system's normal operation,
		rather than looking for signs of known-to-be-malicious
		activity.
		
			\subsubsection{Intrusion Detection Systems}
			There are two approaches to analyzing of events using IDSs. These are misuse-based and anomaly-based approaches. Mis-
			use-based IDSs aim to distinguish events that violate system policy. Anomaly-based IDSs try analyzing abnormal activities
			and flag these activities as attacks. Both approaches have advantages and disadvantages when compared to each other.
			
			Snort is the most commonly used signature-based intrusion detection system. Snort is a network intrusion detection system that runs over IP networks analyzing real-time traffic for detection of misuses.
			Snort depends on a template-matching scheme and makes content analysis. It has the ability to flag alerts depending on pre-defined misuse rules and saves packets in tcpdump files or in plain text files. Snort is preferred to be used in academic research projects as it is an open-source tool and for this reason we have also chosen Snort as the signature-based intrusion detection system in our work.
			
			Anomaly detection based intrusion detection systems are separated into many sub-categories in the literature including
			statistical methodologies, data mining, artificial neural networks, genetic algorithms and immune systems. Among these sub-categories, statistical methods are the most commonly used ones in order to detect intrusions by analyzing abnormal activities occurring in the network.
			
			In this project we have used neural networks based to detect abnormal traffic in the network and then to the host. In our approach we sniff the packets going from and to the host and then apply feature extraction on those packets to get features about the traffic. Then it is converted into a feature matrix which is then fed into the neural network which adjusts its weights accordingly.
			
			\subsubsection{Misuse-based IDS}
			Misuse detectors analyze system activities and try to find a match between these activities and known attacks having
			definitions or signatures introduced to the system beforehand.
			
			\textit{Advantages}
			\begin{itemize}
				
				\item Misuse detectors are very efficient in detecting attacks without signalling false alarms (FA).
				
				\item Misuse detectors can quickly detect specially designed intrusion tools and techniques.
				\item Misuse detectors provide systems administrators an easy to use tool to monitor their systems even if they are not security
				experts.
			\end{itemize}
			
			\textit{Disadvantages}
			\begin{itemize}
				\item Misuse detectors can only detect attacks known beforehand. For this reason the systems must be updated with newly discovered attack signatures.
				
				\item Misuse detectors are designed to detect attacks that have signatures introduced to the system only. When a well-known attack is changed slightly and a variant of that attack is obtained, the detector is unable to detect this variant of the same attack.
			\end{itemize}
		
			\subsubsection{Anomaly-based IDSs}
			Anomaly detectors detect behaviors on a computer or computer network that are not normal. According to this ap-
			proach, behaviors deviating from behaviors assumed as ‘‘normal” are thought to be attacks and anomaly detectors com-
			pute the deviation in order to detect these attacks. Anomaly detectors construct profiles of users, servers and network
			connections using their normal behaviors. These profiles are produced using the data that is accepted as normal. After
			the profile construction, detectors monitor new event data, compare the new data with obtained profile and try to detect
			deviations. These deviations from normal behaviors are flagged as attacks. Pros and cons of anomaly-based approach are as follows\\
			\textit{Advantages}
			\begin{itemize}
				\item Anomaly-based IDSs, superior to signature-based ones, are able to detect attacks even when detailed information of the attack does not exist.
				\item Anomaly-based detectors can be used to obtain signature information used by misuse-based IDS.
			\end{itemize} 
		
			\textit{Disadvantages}
			\begin{itemize}
				\item Anomaly-based IDSs generally flag many false alarms (FA) just because user and network behavior are not always known beforehand.
				\item Anomaly-based approach requires a large set of training data that consist of system event log in order to construct normal behavior profile.
			\end{itemize}
			
		\subsection{Goal}\label{intro:goal}
		Our project is developed with the following goals in mind:
		
		\begin{itemize}
			\item Handle known classes of attacks.
			\item Handle unknown classes of attacks.
			\item Operate in real-time to classify continuous network traffic.
			\item Retrain the model based on current network traffic without disrupting the real-time analysis of traffic.
		\end{itemize}
	
		For fulfilling above goals we have developed a neural network based on a very simple Softmax Regression model. It classifies the network packets based on labels and tries to identify the category of the packet according to the feature vector.
		
		Features are selected according to the feature selection algorithm and extracted in real-time from the packets using Python v3.6.1 then fed into the pre-trained neural network. This model then evaluates the packet and classifies it as one of the predefined attacks or a new attack or gives it a normal class.
		
		\subsection{Document Structure}
		This report is organized as follows. % TODO!
	% Introduction Ends
	
	
	% Networking
	\cleardoublepage
	\section{Networking}\label{sec:netwrk}
	\textbf{Ever wondered how a message transfers from one device to another?}\\ 
	
	Well,   the   answer   is   an   OSI   model   itself.   An   Open Systems  Interconnection  (OSI)  reference  model  is  the world’s major used networking architecture model.
	
		\subsection{OSI Layer}
		``The  OSI  reference  model  adopts  a  layered  approach 
		where  a  communication subsystem  is  broken  down  into seven  layers,  each  one  of  which  performs  a well-defined function.''
		
		 First, TCP traffic is “real world,” since TCP is widely used in today' s Internet.  Consequently, any network path properties we can derive from measurements of a TCP transfer can potentially be directly applied to tuning TCP performance.  Second, TCP packet streams  allow  fine-scale  probing  without  unduly  loading  the  network, since TCP adapts its transmission rate to current congestion levels.
	 
		\subsection{Types of Protocols}
			When capturing packets from a network device we encounter the following types of protocols.
			
			\subsubsection{Transmission Control Protocol(TCP/IP)}
			It  is  most  widely  used network  protocol.HTTP  is  a  server/client  based  protocol used to transfer web pages on network.
			TCP/IP is a stack of protocols having different protocols on both layer 3 and 4. It   is   a   layer 4 protocol and provide bi-directional communication. IP is a layer 3 protocol  and  provides addressing system that communication on  network. TCP/IP is a three way handshake process which first sends SYN then the server responds with SYN and ACK then finally the client returns ACK to the server. This is called three way handshake in TCP.
			
			Thus, in TCP, a connection is established before any data transfer takes place. Transmission Control Protocol accepts data from a data stream, divides it into chunks, and adds a TCP header creating a TCP segment. The TCP segment is then encapsulated into an Internet Protocol (IP) datagram, and exchanged with peers.
			
			A TCP segment consists of a segment header and a data section. The TCP header contains 10 mandatory fields, and an optional extension field.
			
			The data section follows the header. Its contents are the payload data carried for the application. The length of the data section is not specified in the TCP segment header. It can be calculated by subtracting the combined length of the TCP header and the encapsulating IP header from the total IP datagram length (specified in the IP header). 
			
			% TODO: Insert TCP Header Image!
			
			\begin{description}
				\item[Source port (16 bits)] --- Identifies the sending port
				\item[Destination port (16 bits)] --- Identifies the receiving port
				\item[Sequence number (32 bits)] --- 
					Has a dual role: 
					\begin{itemize}
						\item If the SYN flag is set (1), then this is the initial sequence number. The sequence number of the actual first data byte and the acknowledged number in the corresponding ACK are then this sequence number plus 1.5
						
						\item If the SYN flag is clear (0), then this is the accumulated sequence number of the first data byte of this segment for the current session.
					\end{itemize}
				\item [Acknowledgment number (32 bits)] ---If the ACK flag is set then the value of this field is the next sequence number that the sender is expecting. This acknowledges receipt of all prior bytes (if any). The first ACK sent by each end acknowledges the other end's initial sequence number itself, but no data.
				
				\item [Data offset (4 bits)] ---
				Specifies the size of the TCP header in 32-bit words. The minimum size header is 5 words and the maximum is 15 words thus giving the minimum size of 20 bytes and maximum of 60 bytes, allowing for up to 40 bytes of options in the header. This field gets its name from the fact that it is also the offset from the start of the TCP segment to the actual data.
				
				\item [Reserved (3 bits)] --- 
				For future use and should be set to zero
				
				\item [Flags (9 bits) (aka Control bits)] ---
				Contains 9 1-bit flags
				\begin{itemize}
					\item NS (1 bit): ECN-nonce concealment protection (experimental: see RFC 3540).
					\item CWR (1 bit): Congestion Window Reduced (CWR) flag is set by the sending host to indicate that it received a TCP segment with the ECE flag set and had responded in congestion control mechanism (added to header by RFC 3168).
					\item ECE (1 bit): ECN-Echo has a dual role, depending on the value of the SYN flag. It indicates:
					\begin{itemize}
						\item If the SYN flag is set (1), that the TCP peer is ECN capable.
						\item If the SYN flag is clear (0), that a packet with Congestion Experienced flag set 
					\end{itemize}
				
					\item (ECN=11) in IP header was received during normal transmission (added to header by RFC 3168). This serves as an indication of network congestion (or impending congestion) to the TCP sender.
					
					\item URG (1 bit): indicates that the Urgent pointer field is significant
					\item ACK (1 bit): indicates that the Acknowledgment field is significant. All packets after the initial SYN packet sent by the client should have this flag set.
					\item PSH (1 bit): Push function. Asks to push the buffered data to the receiving application.
					\item RST (1 bit): Reset the connection
					\item SYN (1 bit): Synchronize sequence numbers. Only the first packet sent from each end should have this flag set. Some other flags and fields change meaning based on this flag, and some are only valid for when it is set, and others when it is clear.
					\item FIN (1 bit): Last package from sender.
				\end{itemize}
				
				\item [Window size (16 bits)] ---
				The size of the receive window, which specifies the number of window size units (by default, bytes) (beyond the segment identified by the sequence number in the acknowledgement field) that the sender of this segment is currently willing to receive (see Flow control and Window Scaling)
				\item [Checksum (16 bits)] ---
				The 16-bit checksum field is used for error-checking of the header and data
				\item [Urgent pointer (16 bits)] ---
				if the URG flag is set, then this 16-bit field is an offset from the sequence number indicating the last urgent data byte
				
			\end{description}
			
			\subsubsection{User Datagram Protocol(UDP)}
			UDP (User Datagram Protocol) is a simple OSI transport layer protocol for client/server network applications based on Internet Protocol (IP). UDP is the main alternative to TCP and one of the oldest network protocols in existence, introduced in 1980.
			
			UDP network traffic is organized in the form of datagrams. A datagram comprises one message unit. The first eight (8) bytes of a datagram contain header information and the remaining bytes contain message data.
			The UDP header consists of 4 fields, each of which is 2 bytes (16 bits).The use of the fields "Checksum" and "Source port" is optional in IPv4. In IPv6 only the source port is optional (see below).
			
			% TODO: Insert UDP Header Image!
			\begin{description}
				\item [Source port number] ---
				This field identifies the sender's port when meaningful and should be assumed to be the port to reply to if needed. If not used, then it should be zero. If the source host is the client, the port number is likely to be an ephemeral port number. If the source host is the server, the port number is likely to be a well-known port number.
				
				\item [Destination port number] ---
				This field identifies the receiver's port and is required. Similar to source port number, if the client is the destination host then the port number will likely be an ephemeral port number and if the destination host is the server then the port number will likely be a well-known port number.
				
				\item [Length] ---
				A field that specifies the length in bytes of the UDP header and UDP data. The minimum length is 8 bytes because that is the length of the header. The field size sets a theoretical limit of 65,535 bytes (8 byte header + 65,527 bytes of data) for a UDP datagram. However the actual limit for the data length, which is imposed by the underlying IPv4 protocol, is 65,507 bytes (65,535 − 8 byte UDP header − 20 byte IP header).In IPv6 jumbograms it is possible to have UDP packets of size greater than 65,535 bytes. RFC 2675 specifies that the length field is set to zero if the length of the UDP header plus UDP data is greater than 65,535.
				
				\item[Checksum] ---
				The checksum field may be used for error-checking of the header and data. This field is optional in IPv4, and mandatory in IPv6. The field carries all-zeros if unused.
			\end{description}
		
			\subsubsection{Internet Control Message Protocol(ICMP)}
			The Internet Control Message Protocol (ICMP) is a supporting protocol in the Internet protocol suite. It is used by network devices, including routers, to send error messages and operational information indicating, for example, that a requested service is not available or that a host or router could not be reached.ICMP differs from transport protocols such as TCP and UDP in that it is not typically used to exchange data between systems, nor is it regularly employed by end-user network applications (with the exception of some diagnostic tools like ping and traceroute)
			
			The ICMP header starts after the IPv4 header and is identified by IP protocol number '1'. All ICMP packets have an 8-byte header and variable-sized data section. The first 4 bytes of the header have fixed format, while the last 4 bytes depend on the type/code of that ICMP packet.
			
			% TODO: Insert ICMP Header.
			
			\begin{description}
				\item [Type ] --- ICMP type
				\item [Code ] --- ICMP subtype
				\item [Checksum] --- Error checking data, calculated from the ICMP header and data, with value 0 substituted for this field. The Internet Checksum is used, specified in RFC 1071.
				\item [Rest of Header] --- Four-bytes field, contents vary based on the ICMP type and code.
			\end{description}
			
	% Networking Ends
	
	% Intrusion Detection	
	\cleardoublepage
	\section{Intrusion Detection}\label{sec:i-detection}
		An intrusion detection system (IDS) is a device or software application that monitors a network or systems for malicious activity or policy violations. Any detected activity or violation is typically reported either to an administrator or collected centrally using a security information and event management (SIEM) system. A SIEM system combines outputs from multiple sources, and uses alarm filtering techniques to distinguish malicious activity from false alarms.
		
		There is a wide spectrum of IDS, varying from antivirus software to hierarchical systems that monitor the traffic of an entire backbone network. The most common classifications are network intrusion detection systems (NIDS) and host-based intrusion detection systems (HIDS). A system that monitors important operating system files is an example of a HIDS, while a system that analyzes incoming network traffic is an example of a NIDS. It is also possible to classify IDS by detection approach: the most well-known variants are signature-based detection (recognizing bad patterns, such as malware) and anomaly-based detection (detecting deviations from a model of "good" traffic, which often relies on machine learning). Some IDS have the ability to respond to detected intrusions. Systems with response capabilities are typically referred to as an intrusion prevention system.
		
		Though they both relate to network security, an IDS differs from a firewall in that a firewall looks outwardly for intrusions in order to stop them from happening. Firewalls limit access between networks to prevent intrusion and do not signal an attack from inside the network. An IDS evaluates a suspected intrusion once it has taken place and signals an alarm. An IDS also watches for attacks that originate from within a system. This is traditionally achieved by examining network communications, identifying heuristics and patterns (often known as signatures) of common computer attacks, and taking action to alert operators. A system that terminates connections is called an intrusion prevention system, and is another form of an application layer firewall.
		\subsection{Types of IDS}
			\subsubsection{Network intrusion detection systems}
			Network intrusion detection systems (NIDS) are placed at a strategic point or points within the network to monitor traffic to and from all devices on the network. It performs an analysis of passing traffic on the entire subnet, and matches the traffic that is passed on the subnets to the library of known attacks. Once an attack is identified, or abnormal behavior is sensed, the alert can be sent to the administrator. An example of an NIDS would be installing it on the subnet where firewalls are located in order to see if someone is trying to break into the firewall. Ideally one would scan all inbound and outbound traffic, however doing so might create a bottleneck that would impair the overall speed of the network. OPNET and NetSim are commonly used tools for simulation network intrusion detection systems. NID Systems are also capable of comparing signatures for similar packets to link and drop harmful detected packets which have a signature matching the records in the NIDS. When we classify the design of the NIDS according to the system interactivity property, there are two types: on-line and off-line NIDS, often referred to as inline and tap mode, respectively. On-line NIDS deals with the network in real time. It analyses the Ethernet packets and applies some rules, to decide if it is an attack or not. Off-line NIDS deals with stored data and passes it through some processes to decide if it is an attack or not.
			
			\subsubsection{Host intrusion detection systems}
			Host intrusion detection systems (HIDS) run on individual hosts or devices on the network. A HIDS monitors the inbound and outbound packets from the device only and will alert the user or administrator if suspicious activity is detected. It takes a snapshot of existing system files and matches it to the previous snapshot. If the critical system files were modified or deleted, an alert is sent to the administrator to investigate. An example of HIDS usage can be seen on mission critical machines, which are not expected to change their configurations.
			
			Intrusion detection systems can also be system-specific using custom tools and honeypots.	
			
		\subsection{Detection Methods}
			\subsubsection{Signature-based}
			Signature-based IDS refers to the detection of attacks by looking for specific patterns, such as byte sequences in network traffic, or known malicious instruction sequences used by malware. This terminology originates from anti-virus software, which refers to these detected patterns as signatures. Although signature-based IDS can easily detect known attacks, it is impossible to detect new attacks, for which no pattern is available.
			\subsubsection{Anomaly-based}
			Anomaly-based intrusion detection systems were primarily introduced to detect unknown attacks, in part due to the rapid development of malware. The basic approach is to use machine learning to create a model of trustworthy activity, and then compare new behaviour against this model. Although this approach enables the detection of previously unknown attacks, it may suffer from false positives: previously unknown legitimate activity may also be classified as malicious.
			
			New types of what could be called anomaly-based intrusion detection systems are being viewed by Gartner as User and Entity Behaviour Analytics (UEBA) (an evolution of the user behaviour analytics category) and network traffic analysis (NTA). In particular, NTA deals with malicious insiders as well as targeted external attacks that have compromised a user machine or account. Gartner has noted that some organizations have opted for NTA over more traditional IDS.
		\subsection{Common Network Attacks}
			Moving up the internet protocol suite, the fundamental problem is similar, there is no authenticity in the messages or confidentiality preserving in most of the mechanisms. This is particularly manifest at the lower level. 
			
			This protocol can be exploited in a surprisingly different number of ways. Now that 24 hours uptime of a service is really important this simply means, a large number of fake connections are enough to make the connection unavailable for the legit users causing widespread trouble. Imagine Facebook servers being unavailable for even 1 hour. When the number of users is this high, this will cause service unavailability for a large number of users. 
			
			\subsubsection{SYN Flodding}
			The SYN Flooding attack is to simply send a lot of SYN packets and never acknoledge any of the replies. This attack was theoretically known to be possible since 80's but came into attention when used to bring down Panix, a New York ISP, for several days in 1996.
			
			\subsubsection{Smurfing}
			The Smurf attack is a distributed denial-of-service attack in which large numbers of Internet Control Message Protocol (ICMP) packets with the intended victim's spoofed source IP are broadcast to a computer network using an IP broadcast address. Most devices on a network will, by default, respond to this by sending a reply to the source IP address. If the number of machines on the network that receive and respond to these packets is very large, the victim's computer will be flooded with traffic. This can slow down the victim's computer to the point where it becomes impossible to work on.
			
			
			\subsubsection{Denial-of-service attack}
			In computing, a denial-of-service attack (DoS attack) is a cyber-attack where the perpetrator seeks to make a machine or network resource unavailable to its intended users by temporarily or indefinitely disrupting services of a host connected to the Internet. Denial of service is typically accomplished by flooding the targeted machine or resource with superfluous requests in an attempt to overload systems and prevent some or all legitimate requests from being fulfilled.
			
			In a distributed denial-of-service attack (DDoS attack), the incoming traffic flooding the victim originates from many different sources. This effectively makes it impossible to stop the attack simply by blocking a single source.
			
			A DoS or DDoS attack is analogous to a group of people crowding the entry door or gate to a shop or business, and not letting legitimate parties enter into the shop or business, disrupting normal operations.
			
			\subsubsection{Neptune Attacks}
			
			Neptune attacks can make memory resources too
			full for a victim by sending a TCP packet requesting  to  initiate a TCP session. This  packet is part of a three-way handshake that is needed to establish a TCP connection between  two  hosts. The SYN flag on this packet is set  to  indicate that a new connection is  to  be  established. This packet includes  a spoofed  source  address, such that the victim is not able to finish the handshake but  had allocated  an  amount of system memory for this connection. After sending many of these packets, the victim eventually runs out of memory resources.
			
			\subsubsection{IPSweep and Portsweep}
			IPsweep  and  Portsweep,  as  their  names  suggest, sweep through IP addresses and port numbers for a  victim  network  and  host  respectively looking for open ports, that could  potentially be used later in an attack.
			
			\subsubsection{Rootkit}
			A rootkit is a collection of computer software, typically malicious, designed to enable access to a computer or areas of its software that would not otherwise be allowed (for example, to an unauthorized user) and often masks its existence or the existence of other software. The term rootkit is a concatenation of "root" (the traditional name of the privileged account on Unix-like operating systems) and the word "kit" (which refers to the software components that implement the tool). The term "rootkit" has negative connotations through its association with malware.
			
			\subsubsection{Satan}
			SATAN (Security Administrator Tool for Analyzing Networks)is a security tool designed by Dan Farmer and Wieste Venema to help systems administrators recognize several network-related security problems, in a world where computer systems are becoming more and more dependant on networks. Satan is a tool designed to probe a computer system for security loopholes, (Security Administrator Tool for Analyzing Networks). Use of Satan is not inherently bad. Network coordinators sometimes run it against their own machines to probe for weaknesses which they can then fix. Running it against a machine you don't own is regarded as a hostile act (and may be illegal under some jurisdictions).
			
		
		\subsection{Development}
		The earliest preliminary IDS concept was delineated in 1980 by James Anderson at the National Security Agency and consisted of a set of tools intended to help administrators review audit trails. User access logs, file access logs, and system event logs are examples of audit trails.
		
		The proposed model uses a Softmax Regression based neural network which classifies the network traffic in real-time.The network is developed in Python using Tensorflow Library (By Google). Next a packet sniffer is developed in python which captures all the packets from and to the host. This form the raw data for the model. Now this data is then preprocessed to form the input to the model. This model then evaluates the network traffic to classify it among the known 18 types or any new attack or normal traffic.
			
		\subsection{Detecting Malicious Activities}
		\lipsum[1-2]
		
		\subsection{Relevant Anomaly Detection Techniques}
		\lipsum[1-2]
			\subsubsection{Network-Based Techniques}
			\lipsum[1-2]
			\subsubsection{Host-Based Techniques}
			\lipsum[1-2]
			\subsubsection{Web-Based Techniques}
			\lipsum[1-2]
	% Intrusion Detection
	
	% Neural Networks
	\cleardoublepage
	\section{Neural Networks}\label{sec:neural-networks}
	\lipsum[1]
		\subsection{What is a Neural Network?}
		\lipsum[1-2]
		\subsection{Types of Neural Networks}
		\lipsum[1-2]
			\subsubsection{Type - 1}
			\lipsum[1-2]
			\subsubsection{Type - 2}
			\lipsum[1-2]
			\subsubsection{Type - 3}
			\lipsum[1-2]
			\subsubsection{Type - 1}
			\lipsum[1-2]
			\subsubsection{Type - 2}
			\lipsum[1-2]
			\subsubsection{Type - 3}
			\lipsum[1-2]
		\subsection{Neural Networks and Intrusion Detection}
		\lipsum[1-3]
	% Neural Networks
	
	% NN-Based IDS
	\cleardoublepage
	\section{Neural Network Based IDS}\label{sec:nn-based-ids}
		\lipsum[2]
		\subsection{Overview}
		\lipsum[2]
		
		\subsection{Datasets}
		\lipsum{3}
		
		\subsection{Raw Data Extraction (Packet Sniffing)}
		\lipsum[1-3]
		
		\subsection{Feature Extraction}
		\lipsum[1-6]
			\subsubsection{Features with small variations}
			\lipsum[1-3]
			\subsubsection{Features with High Correlation}
			\lipsum[1-3]
			
		\subsection{Implementations}
		\lipsum[1]
			\subsubsection{Supervised Learning Algorithm}
			\lipsum[1]
			\subsubsection{Single NN Classification Model}
			\lipsum[1]
			\subsubsection{Three-Parallel Neural Network Classification}
			\lipsum[1-2]
		\subsection{Proposed Model: Softmax Regression-based Neural Network}
		\lipsum[1-4]
	% NN-Based IDS	
	
	
	%Experimental Methodology
	\cleardoublepage
	\section{Experimental Methdology}
	\lipsum[1]
		\subsection{Dataset}
		\lipsum[1-2]
		\subsection{Host Configuration}
		\lipsum[1-2]
		\subsection{Network Configuration}
		\lipsum[1-3]
		\subsection{Gathering Raw Data}
		\lipsum[1]
			\subsubsection{From Packet Sniffing}
			\lipsum[1-2]
			\subsubsection{PCAP Files from the Internet \cite{Chianese2017}}
			\lipsum[1-2]
	%Experimental Methodology Ends
	
	% Results and Discussion
	\cleardoublepage
	\section{Results and Discussion}
	\lipsum[1]
		\subsection{Model 1}
		\lipsum[1-3]
		\subsection{Model 2}
		\lipsum[1-3]
		\subsection{Model 3}
		\lipsum[1-3]
		\subsection{Model 4}
		\lipsum[1-3]
		\subsection{Model 5}
		\lipsum[1-3]	
	% Results and Discussion Ends

	% Conclusion and Future Scope
	\cleardoublepage
	\section{Conclusion and Future Scope \cite{AlEroud2017}}
	\lipsum[1-4]
	% Conclusion and Future Scopes
	
	% References and Bibliography
	\cleardoublepage
	\bibliographystyle{IEEEtran}
	\addcontentsline{toc}{section}{\numberline{}References}
	\bibliography{references/bib}
	% References and Bibliography
		
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
\end{document}